{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Side notes \n",
    "_(code snippets, summaries, resources, etc.)_\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Summary of topics covered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![summary of feature selection](feature_selection_images/summary_feature_selection.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Feature Selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Knowledge Discovery\n",
    "    - Interpretatibility and insight\n",
    "2. Curse of Dimensionality\n",
    "    - Samples required increases exponential as features increase, that is $2^{n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Quiz: How hard is the problem?__\n",
    "\n",
    "![feature selection algorithms quiz](feature_selection_images/fs_algorithms_quiz.png)\n",
    "\n",
    "- Used combinatorics to determine solution\n",
    "    - Must try all subsets, and there are an exponential number of subsets $m$\n",
    "    - if we do not know anything about $m$, combinations are $2^{n}$\n",
    "    - if we need $m$ to be half or less of $n$, for example, then we use ${n \\choose m}$ (which is also exponential)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of feature selection algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Filtering\n",
    "    - Search criterion is independent of the learning algorithm\n",
    "- Wrapping\n",
    "    - Search criterion is dependant on the learning algorithm\n",
    "    \n",
    "![types of feature selection algorithms](feature_selection_images/types_of_fs_algorithms.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of each feature selection algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![feature selection algorithm tradeoff](feature_selection_images/fs_algorithms_tradeoff.png)\n",
    "\n",
    "__Filtering:__\n",
    "- the price of faster performance of this type of algorithm is that features are considered in isolation\n",
    "    - i.e. does not take into account relationships between features that might make a particular feature more relevant\n",
    "- Possible search criteria include:\n",
    "    - information gain as seen in DTs\n",
    "    - variance, entropy, the gini index\n",
    "    - \"useful\" features (e.g. as determine by neural nets by assignment weights to different features)\n",
    "    - Independent / non-redundant\n",
    "    - other statistical measures of relevant\n",
    "- Filtering could take into account labels for the samples\n",
    "    - e.g. entropy would not consider labels whereas\n",
    "    - information gain would\n",
    "- A Decision Tree's information gain can be used as the criterion with which to filter features, i.e. finding the features that provide the most information given the class label\n",
    "    - A DT learner by definition, provides a subset of features (features that it decided to split on, ones that are most important for predicting the right labels).\n",
    "    - A union of these features could then be passed to another type of learner.\n",
    "    - In that case, we would utilize the inductive bias of a DT to choose the features that are most important for predicting the label, and the inductive bias of antoher learner to do the actual learning.\n",
    "    - E.g. KNN's problem with dimensionality could be offset by a DTs strength in that area\n",
    "\n",
    "__Wrapping:__\n",
    "- Use the specific learner to assess relevance of features to avoid searching through all possible combinations of features (exponential time cost)\n",
    "- Possible criteria for choosing with features to run through learner:\n",
    "    - Hill climbing (others look a lot like this one)\n",
    "    - Randomized optimization\n",
    "    - Forward sequential selection (polynomial performance; a kind of hill climbing)\n",
    "        1. Choose best single feature, running through each individually through the learner\n",
    "        - Determine the best feature that runs in combination with the previously selected feature(s)\n",
    "        - Once an addition of a feature does not decrease error significantly, stop.\n",
    "    - Backward elimination (another kind of hill climbing)\n",
    "        - Similar algorithm to forward search except starts with all features and eliminated one-by-one until next elimination would significantly decrease the learner's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz: Minimum features required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Minimum features required quiz](feature_selection_images/min_features_quiz.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevance vs Usefulness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Usefulness measures effect of a variable on a particular learning algorithm\n",
    "    - This is ultimately what we care about\n",
    "- Relevance is about information\n",
    "    - A variable could be \"strongly\" or \"weakly\" relevant (definitions below)\n",
    "    - A feature is irrelevant if it does not add any information to the classifier\n",
    "    - A subset of usefulness, measuring the usefulness of a variable with respect to the Bayes Optimal Classifier (see note on this below).\n",
    "\n",
    "For more explanation on the slides below, see [these](https://classroom.udacity.com/nanodegrees/nd009/parts/0091345407/modules/542278935775460/lessons/5415378701/concepts/6010086150923#) [videos](https://classroom.udacity.com/nanodegrees/nd009/parts/0091345407/modules/542278935775460/lessons/5415378701/concepts/6010086160923)\n",
    "\n",
    "![relevance](feature_selection_images/relevance.png)\n",
    "\n",
    "![relevance vs usefulness](feature_selection_images/relevance_vs_usefulness.png)\n",
    "\n",
    "\n",
    "Note about Bayes Optimal Classifer (B.O.C.):\n",
    "- It is theoretical concept, resulting classifier if we were able to test every possible one (an infinite number).\n",
    "- Truly a measure of information of variables\n",
    "- Any other algorithm has an inductive bias\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py2]",
   "language": "python",
   "name": "conda-env-py2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
