{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Side notes\n",
    "_(code snippets, summaries, resources, etc.)_\n",
    "- Provides extra explanation: [_Introduction to Boosting_ PDF by Udactiy](https://www.evernote.com/shard/s37/nl/1033921335/ea429564-f35b-4e92-81c0-9a7a1860fc06/) (Evernote)\n",
    "    - Some notes below come from the PDF\n",
    "- Can also see sections 10.1, 10.3 and 10.5 from Ch. 10 in \n",
    "    - [Elements of Statistical Learning: Data Mining, Inference, and Prediction 2nd Ed. by Trevor Hastie, Robert Tibshirani Â & Jerome Friedman (2013)](https://www.evernote.com/shard/s37/nl/1033921335/9dbcbee9-a0b0-4aad-a95b-0acf6cebaf6d/) (Evernote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning: Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of topics covered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![summary](ensemble_learning_boosting_images/boosting_summary.png)\n",
    "\n",
    "- Boosting is agnostic to the learner, so long as it is a weak learner.\n",
    "- Looked at what error really means with respect to some underlying distribution $D$\n",
    "- In practice, over time, as a boosting algorithm lowers its bias, its variance does not increase, but rather _decreases_ as well. (Sounds too good to be true, but it isn't!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An example of additive expansion'\n",
    "- Originally designed for classification tasks (focus here), but can also be applied to regression\n",
    "- AdaBoost is an _agnostic_ learner\n",
    "    - Only requirement is that the base learner must consistently (with high probability) achieve greater performance than random guessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing a Weak Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Definition:__ Weak learner\n",
    "- Formal definition: $\\forall_{D}$  $P_{D}[.] \\leqslant \\frac{1}{2} - \\varepsilon$\n",
    "    - i.e. has an expected error greater than half.\n",
    "    - $\\varepsilon$, often used in ML, is some really really small number\n",
    "\n",
    "So long as you can consistently beat random guessing, any true boosting algorithm will be able to increase the accuracy of the final ensemble.\n",
    "\n",
    "What weak learner you should choose is then a trade off between 3 factors:\n",
    "\n",
    "1. The bias of the model. \n",
    "    - A lower bias is almost always better, but you don't want to pick something that will overfit (yes, boosting can and does overfit)\n",
    "- The training time for the weak learner. \n",
    "    - Generally we want to be able to train a weak learner quickly, as we are going to be building a few hundred (or thousand) of them.\n",
    "- The prediction time for our weak learner. \n",
    "    - If we use a model that has a slow prediction rate, our ensemble of them is going to be a few hundred times slower!\n",
    "\n",
    "The classic weak learner is a decision tree. \n",
    "- By changing the maximum depth of the tree, you can control all 3 factors. \n",
    "- This makes them incredibly popular for boosting. \n",
    "- What you should be using depends on your individual problem, but decision trees is a good starting point.\n",
    "\n",
    "NOTE: So long as the algorithm supports weighted data instances, any algorithm can be used for boosting. E.g. \"A guest speaker at my University was boosting 5 layer deep neural networks for his work in computational biology.\"\n",
    "\n",
    "From [StackOverflow](http://stackoverflow.com/questions/20435717/what-is-a-weak-learner): What is a weak learner?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging ensemble learning technique (example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ensemble learning example](ensemble_learning_boosting_images/ensemble_learning_example.png)\n",
    "\n",
    "Bagging a.k.a. bootstrap aggregation\n",
    "\n",
    "__Key:__\n",
    "- Red data points are training data; green are testing data\n",
    "- Dotted lines are components of the final regression\n",
    "- Red line is average of third order polynomials run on random subsets\n",
    "- Blue line is the result of a regression run once on all training data (for comparison)\n",
    "\n",
    "__Method:__\n",
    "- Pick 5 random subsets of 5 example points each (random with replacement)\n",
    "- A 3rd order polynomial regression is trained on each subset\n",
    "- Finally, the 5 regressions are averaged to produced a polynomial (also 3rd order)\n",
    "\n",
    "__Result:__\n",
    "- Averaging regression does a better job discovering underlying structure fo data\n",
    "    - lessens likelihood of over-fitting by not being mislead by any individual data point (same reason for doing cross-validation)\n",
    "    - don't get trapped by data that is wrong due to noise\n",
    "    - \"Averages out all the variances of the differences\"\n",
    "- In practice, bagging technique is particularly effective at avoiding over-fitting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Instead of choosing subsets randomly, emphasize the \"hardest\" examples\n",
    "    - Done by using a weighted vote\n",
    "\n",
    "__Definition of error:__\n",
    "- $P_{D}[h(x) \\neq c(x)]$\n",
    "- i.e. the probability given the underlying distribution that the hypothesis will disagree with the true concept on some particular instance $x$\n",
    "- Depends on the distributions of different types of data points\n",
    "- Not the number of distinct possible mistakes but the number of times these mistakes occurs across the distribution of the data\n",
    "- More common examples would be more important to learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting in pseudocode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![boosting in pseudocode](ensemble_learning_boosting_images/boosting_in_pseudocode.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![boosting formula 1](ensemble_learning_boosting_images/boosting_formula_1.png)\n",
    "\n",
    "- $z_{t}$ is \"whatever normalization constant at time $t$ in order to make it all work out to be a distribution\"\n",
    "- Answers is depends, but if some other examples disagree, then this example agreeing will decrease $D_{t}(i)$, i.e. the distribution of $i$ at $t$.\n",
    "- And vice versa, if there is at least one example that agrees, an example that disagrees will increase $D_{t}(i)$\n",
    "- Mathematically represents the idea that if a particular example is wrong, it will be weighted higher, i.e. it is presumed to be harder.\n",
    "\n",
    "![boosting formula 1](ensemble_learning_boosting_images/boosting_formula_2.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting Example: Three Little Boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![boosting example 1](ensemble_learning_boosting_images/boosting_example_1.png)\n",
    "\n",
    "![boosting example 2](ensemble_learning_boosting_images/boosting_example_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When Boosting Overfits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![boosting overfitting quiz](ensemble_learning_boosting_images/boosting_overfitting_quiz.png)\n",
    "\n",
    "- If the underlying learner overfits and it will always overfit even during the boosting algorithm, the boosted algorithm will be overfit\n",
    "    - A.N.N. is already prone to overfitting due to many parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example code of algorithm\n",
    "From [forked GitHub repo](https://github.com/mdlynch37/AdaBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/mdlynch37/projects/coding/data_science_algorithms/Classifiers')\n",
    "from AdaBoost import usage, AdaBoost\n",
    "# %run AdaBoost/AdaBoost.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "#  AdaBoost.py\n",
    "#  \n",
    "#  Copyright 2015 Overxflow \n",
    "#  \n",
    "import sys\n",
    "sys.path.append('/Users/mdlynch37/projects/coding/data_science_algorithms/Classifiers')\n",
    "\n",
    "from Classifiers import K_nearest_neighbour\n",
    "from Classifiers import Multinomial\n",
    "from Classifiers import Perceptron\n",
    "from Classifiers import KernelPerceptron\n",
    "from math import log,floor,e\n",
    "from os import system,name\n",
    "try: import cPickle as pickle\n",
    "except: import pickle\n",
    "from sys import argv\n",
    "\n",
    "def cls(): system(['clear','cls'][name == 'nt'])\n",
    "    \n",
    "def header():\n",
    "\tprint \"\"\"\n",
    "   _   _   _   _   _   _   _   _  \n",
    "  / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ \n",
    " ( A | d | a | B | o | o | s | t )\n",
    "  \\_/ \\_/ \\_/ \\_/ \\_/ \\_/ \\_/ \\_/ \n",
    "          By Overxfl0w13\n",
    "\"\"\"\n",
    "\n",
    "def footer(result_file=None): print \"[END] Process finished without saving results.\\n\" if result_file==None else \"[END] Process finished, saved classified in file \"+result_file+\". \\n\"\n",
    "\n",
    "def usage():\n",
    "\tprint \"\"\"\n",
    "    Usage: AdaBoost.py train_data_file iterations [classify] [test_data_file] [output_file] \\n\\n\\\n",
    "    \\ttrain_data_file -> Name of file with train data\\n\\\n",
    "    \\titerations      -> process iterations\\n\\\n",
    "    \\tclassify        -> Optional [YES-NO], specifies if you want to classify test data\\n\\\n",
    "    \\ttest_data_file  -> Optional, only if you want to classify, specifies name of file with test data\\n\\\n",
    "    \\toutput_file     -> Optional, specifies destination file\\n\"\n",
    "    \"\"\"\n",
    "def AdaBoost(samples,M):\n",
    "\tweight_samples    = [1.0/len(samples) for sample in samples]\n",
    "\tclassifiers       = [K_nearest_neighbour,Multinomial,Perceptron,KernelPerceptron]\n",
    "\tclassifiers_error = [0 for x in classifiers]\n",
    "\tfinal_classifier  = []\n",
    "\tfor it in xrange(M):\n",
    "\t\tbest_classifier       = K_nearest_neighbour # Random #\n",
    "\t\tindex_best_classifier = 0 # Random #\n",
    "\t\tindex_sample          = 0\n",
    "\t\tcomputed_classes = [[] for classifier in classifiers]\n",
    "\t\tfor sample in samples:\n",
    "\t\t\tcclass = sample[1]\n",
    "\t\t\tsample = sample[0]\n",
    "\t\t\tindex_classifier = 0\n",
    "\t\t\tfor classifier in classifiers:\n",
    "\t\t\t\tcomputed_class = classifier.classify(samples,sample)\n",
    "\t\t\t\tcomputed_classes[index_classifier].append(computed_class)\n",
    "\t\t\t\tif computed_class != cclass: classifiers_error[index_classifier] += weight_samples[index_sample]\n",
    "\t\t\t\tindex_classifier += 1\n",
    "\t\t\tindex_sample += 1\n",
    "\t\t# Calcular el mejor clasificador (menor error) #\n",
    "\t\tmin_error = min(classifiers_error)\n",
    "\t\tindex_best_classifier = classifiers_error.index(min_error)\n",
    "\t\tbest_classifier = classifiers[index_best_classifier]\n",
    "\t\t# Recalcular peso del clasificador #\n",
    "\t\talpha_best_classifier = (1.0/2)*log((1-min_error)/(min_error+(1.0/10**20)))\n",
    "\t\t# Configurar clasificador de la iteracion actual #\n",
    "\t\tfinal_classifier.append((alpha_best_classifier,best_classifier))\n",
    "\t\t# Si el error > 0.5 parar #\n",
    "\t\tif min_error>0.5 or min_error==0:  print \"[!] Min error with only 1 classifier.\\n\";  return final_classifier\n",
    "\t\t# Recalcular pesos de las muestras #\n",
    "\t\tindex_sample = 0\n",
    "\t\tfor sample in samples:\n",
    "\t\t\tcclass = sample[1]\n",
    "\t\t\tsample = sample[0]\n",
    "\t\t\tweight_samples[index_sample] = weight_samples[index_sample]*(e**(-cclass*alpha_best_classifier*computed_classes[index_best_classifier][index_sample]))\n",
    "\t\t# Normalizar pesos de las muestras #\n",
    "\t\tindex_sample  = 0\n",
    "\t\ttotal_weights = sum(weight_samples) \n",
    "\t\tweight_samples = map(lambda x:float(x)/sum(weight_samples),weight_samples)\n",
    "\treturn final_classifier\n",
    "\n",
    "def load_data(filename):\n",
    "\ttry:\n",
    "\t\twith open(filename,'rb') as fd: obj = pickle.load(fd)\n",
    "\t\tfd.close()\n",
    "\t\treturn obj\n",
    "\texcept IOError as ie: print \"[-] File\",filename,\" doesn't exist.\\n\"; exit(0)\n",
    "\t\n",
    "def save_object(object,dest):\n",
    "\twith open(dest,'wb') as fd: pickle.dump(object,fd,pickle.HIGHEST_PROTOCOL)\n",
    "\tfd.close()\t\n",
    "\t\n",
    "def classify_boost(final_classifier,samples,sample):\n",
    "\tval = 0\n",
    "\tfor item in final_classifier: val = item[0]*item[1].classify(samples,sample)\n",
    "\treturn -1 if val<0 else 1\n",
    "\n",
    "def classify_file(final_classifier,samples,test_samples,output_file):\n",
    "\twith open(output_file,\"w\") as fd:\t\n",
    "\t\tfd.write(\"\"\"   _   _   _   _   _   _   _  \n",
    "  / \\ / \\ / \\ / \\ / \\ / \\ / \\ \n",
    " ( R | e | s | u | l | t | s )\n",
    "  \\_/ \\_/ \\_/ \\_/ \\_/ \\_/ \\_/\\r\\n\\r\\n\\r\\n\"\"\")\n",
    "\t\tfd.write(stringify_classifier(final_classifier)+\"\\r\\n\")\t\n",
    "\t\tfor sample in test_samples: fd.write(\"Sample \"+str(sample)+\" classified in: \"+str(classify_boost(final_classifier,samples,sample))+\"\\r\\n\")\n",
    "\tfd.close()\n",
    "    \t\n",
    "def stringify_classifier(final_classifier):  \n",
    "\tst = \" -> \"\n",
    "\tfor item in final_classifier: st += str(item[0])+\"*\"+item[1].__str__()+\"(x)+\"\n",
    "\treturn st[:-1]\n",
    "\t\n",
    "def __str__(final_classifier):\n",
    "\tprint \"Classifier\\n\".center(80)\n",
    "\tprint \"----------\\n\".center(80)\n",
    "\tst = stringify_classifier(final_classifier)\n",
    "\tprint \"\\n\"+st+\"\\n\"\n",
    "\t\n",
    "# if __name__ == \"__main__\":\n",
    "    \n",
    "# \tcls()\n",
    "# \theader()\n",
    "# \tif len(argv)<3: usage();exit(0)\n",
    "# \tif len(argv)!=3:\n",
    "# \t\tif len(argv)!=6 or argv[3].lower() not in [\"yes\",\"no\"]: usage();exit()\n",
    "# \ttrain_data_file = argv[1]\n",
    "# \titerations      = int(argv[2])\n",
    "# \tif len(argv)>3:\n",
    "# \t\tclassify        = argv[3]\n",
    "# \t\ttest_data_file  = argv[4]\n",
    "# \t\toutput_file     = argv[5]\n",
    "# \ttrain_samples = load_data(train_data_file)\n",
    "# \tfinal_classifier = AdaBoost(train_samples,iterations)\n",
    "# \tif len(argv)>=3 and argv[3].lower()==\"yes\":\n",
    "# \t\ttest_samples  = load_data(test_data_file) # Test with same train data, ... VERY OPTIMISTIC!! #\n",
    "# \t\tclassify_file(final_classifier,train_samples,test_samples,output_file)\n",
    "# \t\t__str__(final_classifier)\n",
    "# \t\tfooter(output_file)\n",
    "# \telse: \n",
    "# \t\t__str__(final_classifier)\n",
    "# \t\tfooter()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
