{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Side notes \n",
    "_(code snippets, summaries, resources, etc.)_\n",
    "\n",
    "__Further reading:__\n",
    "- [_Neural Networks_ PDF worksheet by Udactiy]( https://www.evernote.com/shard/s37/nl/1033921335/50316007-f4a1-430e-a914-db8458a7830d/) in Evernote\n",
    "- [_Gradient Descent - Problem of Hiking Down a Mountain_ PDF worksheet by Udactiy]( https://www.evernote.com/shard/s37/nl/1033921335/f754539a-a88e-4ac1-85f3-dd5d705e4d37/) in Evernote\n",
    "- Calculus used for Sigmoid Function below is explained at [WolframMathWorld](http://mathworld.wolfram.com/SigmoidFunction.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "## Summary of topics covered\n",
    "![summary of neural networks](neural_networks_images/summary_neural_networks.png)\n",
    "\n",
    "## Perceptrons\n",
    "- Type of _neural net unit_\n",
    "\n",
    "![neural network in brain](neural_networks_images/neural_network_brain.png)\n",
    "\n",
    "![artificial neural network](neural_networks_images/artificial_neural_network.png)\n",
    "\n",
    "### Power of a perceptron unit\n",
    "![perceptron power](neural_networks_images/perceptron_power.png)\n",
    "\n",
    "- Generalized to _halfplanes_\n",
    "- Perceptrons will always be linear functions that compute hyperplanes\n",
    "\n",
    "### Boolean logic with perceptrons\n",
    "- Perceptrons with certain combinations of weights and inputs act behave as a kind of \"logic gate\"\n",
    "- These perceptrons can be combined to represent any boolean operator\n",
    "- Particularly helpful for overcoming decision tree's problem with parity, i.e. `XOR` operator (see below)\n",
    "\n",
    "![perceptron boolean AND](neural_networks_images/perceptron_and.png)\n",
    "\n",
    "![perceptron boolean OR](neural_networks_images/perceptron_or.png)\n",
    "\n",
    "![perceptron boolean NOT](neural_networks_images/perceptron_not.png)\n",
    "\n",
    "![perceptron boolean XOR](neural_networks_images/perceptron_xor.png)\n",
    "\n",
    "## Training Neural Networks\n",
    "- That is, _given examples_, find weights that map inputs to outputs\n",
    "- Rules for training covered below are\n",
    "    1. Perceptron rule (with thresholds)\n",
    "    - Gradient descent or delta rule (unthresholded)\n",
    "\n",
    "### Perceptron rule\n",
    "- When output _is_ thresholded\n",
    "- If data is _linearly separable_, the algorithm below will find it! (in a finite number of iterations).\n",
    "- Algorithm has to be terminated when the weight value is no longer changed at each iteration, i.e. `actual y == y-hat`\n",
    "- It can be hard to tell if data is linearly separable, especially with lots of dimensions\n",
    "- If this algorithm does not terminate for a while, this could mean data is not linearly separable, but since _finite_ could be any number, we cannot be certain of that.\n",
    "    - \"if we could solve the halting problem, we could solve this, but not necessarily so that problem could be solved another way...\"\n",
    "\n",
    "\n",
    "\n",
    "![perceptron rule calculation part 1](neural_networks_images/perceptron_rule_calc_1.png)\n",
    "![perceptron rule algorithm part 2](neural_networks_images/perceptron_rule_calc_2.png)\n",
    "![perceptron rule algorithm part 3](neural_networks_images/perceptron_rule_calc_3.png)\n",
    "\n",
    "\n",
    "----------------------------\n",
    "\n",
    "### Gradient descent or delta rule\n",
    "- When output _is not_ thresholded\n",
    "- Most robust to data set that are not linearly separable\n",
    "    - converges to the limit of the local optimum\n",
    "- Relies on calculus to minimize the error, i.e. change the weights to push the error down\n",
    "    - 1/2 in equation does not affect outcome but it makes result of partial derivative calculation cleaner.\n",
    "\n",
    "![gradient descent calculation](neural_networks_images/gradient_descent_calc.png)\n",
    "\n",
    "#### Perceptron rule vs. gradient descent\n",
    "\n",
    "![perceptron rule vs gradient descent](neural_networks_images/perceptron_vs_gradient_d.png)\n",
    "\n",
    "### Sigmoid unit\n",
    "- Hack on the gradient descent equation that allows `y-hat` to be substituted and differentiated instead of `a`.\n",
    "- Uses a _sigmoid function_ to force this jump into a differentiable threshold\n",
    "- Calculus used to get to final equation below can be explored at [WolframMathWorld - Sigmoid Function](http://mathworld.wolfram.com/SigmoidFunction.html)\n",
    "\n",
    "![sigmoid for differentiable threshold](neural_networks_images/sigmoid.png)\n",
    "\n",
    "### Back propogation in Neural Networks \n",
    "- \"A computationally beneficial organization of the chain rule.\"\n",
    "- Convenient method to compute derivatives with repsect to all the different weights in the network\n",
    "- Network learns through:\n",
    "    - Information flows from inputs to outputs\n",
    "    - Then, error information flows back from the outputs to the inputs\n",
    "- Could also be called _error back propogation_\n",
    "- Can be applied to units of another differentiable function\n",
    "- Error function, in this case some of least squares, can have multiple \"local\" optima / minima\n",
    "    - a single unit's error function will have one local optimum, bottom of one parabola, but globally multiple parabolas are combined from all units.\n",
    "\n",
    "![back propogation](neural_networks_images/back_propogation.png)\n",
    "\n",
    "### Optimizing Weights, brief intro\n",
    "- Techniques to solve problem of multiple local optima, which will cause algorithm to get stuck in one minima even if it is not the global optima.\n",
    "- for image below: red bullet points are aspect that add to a model's complexity\n",
    "\n",
    "![opitmizing weights](neural_networks_images/optimizing_weights.png)\n",
    "\n",
    "### Restriction Bias\n",
    "__definition__ of restriction bias:\n",
    "- Describes the _representational power_ of a particular data structure, e.g. of a network of neurons\n",
    "- Restricts the hypotheses that will be considered\n",
    "\n",
    "#### Evaluating restriction bias\n",
    "- _perceptron unit:_ linear, only considering planes\n",
    "    - to _Networks of perceptrons:_ allows boolean functions like `XOR`\n",
    "    - to _Networks of units with sigmoids & other arbitrary functions:_ allows lots of layers and nodes that can become much more complex, not many restrictions at all\n",
    "- Neural networks can represent _any_ mapping of inputs to outputs, like:\n",
    "    - _boolean:_ with network of threshold-like units\n",
    "    - _continuous:_ as long as smooth curves, connected / no jumps\n",
    "        - using single hidden layer of nodes\n",
    "        - each node covers some portion of function\n",
    "        - nodes are then \"stitched together\" to give output\n",
    "    - _arbitrary:_ functions that aren't continuous\n",
    "        - requires two hidden layers\n",
    "        - with additional hidden layer, output can be stitched together even with gaps in the function.\n",
    "        \n",
    "#### Overfitting\n",
    "- Danger of overfitting neural network can even represent noise in our training set\n",
    "- To solve this, restrict number of hidden nodes and layer in network\n",
    "    - Neural network can only capture as much of a function as its bounds allow\n",
    "    - i.e. the particular network architecture can have restrictions even though an unbounded neural network will not.\n",
    "- Other solutions are ones that are applied to other learners like:\n",
    "    - Cross validation to decide how many nodes per layer, how large weights can get before stopping. \n",
    "- Complexity of a neural network is not only in the nodes and layers, but also in its weights, i.e. how _much_ it is trained\n",
    "\n",
    "![restriction bias](neural_networks_images/restriction_bias.png)\n",
    "\n",
    "### Preference Bias\n",
    "__definition__ of preference bias:\n",
    "- Characteristics that determine whether one subclass of algorithm would be selecteed over another.\n",
    "    - e.g. preferred decision trees are correct ones, one with top nodes having the most information gain, ones that aren't longer than necessary, etc.\n",
    "\n",
    "#### Evaluating preference bias\n",
    "- For _neural networks with gradient descent:_\n",
    "    - prefers models with lower complexity (Occam's razor)\n",
    "        \n",
    "![preference bias](neural_networks_images/preference_bias.png)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
