{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Side notes \n",
    "_(code snippets, summaries, resources, etc.)_\n",
    "- See Evernote note on Decision Trees in Intro to ML (lesson 3)\n",
    "    - Entropy\n",
    "    - Information gain\n",
    "    - Pros and Cons of using DTs\n",
    "- See below for definitions of key terms in classification learning\n",
    "- Further reading: [ID3 Algorithm PDF worksheet by Udactiy]( https://www.evernote.com/shard/s37/nl/1033921335/ca586392-9b96-4e7c-9fba-541d02af5c3a/) in Evernote\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "## Summary of topics covered\n",
    "- ID3: A top down learning algorithm\n",
    "- Expressiveness of DTs\n",
    "- Bias of ID3\n",
    "- \"Best\" attributes / GAIN(S, A)\n",
    "- Preventing overfitting\n",
    "\n",
    "## Classification vs. Regression\n",
    "**Classification:**\n",
    "- For output that is discrete set\n",
    "    - usually small number of possibible labels\n",
    "    - applies to numbers whose values do not have any ordinal meaning (i.e. telephone numbers)\n",
    "\n",
    "**Regression:**\n",
    "- For output that is continuous\n",
    "    - applies \n",
    "    \n",
    "**Either Regression or Classification:**\n",
    "Person's Age:\n",
    "- could be either, depends upon exactly how output is defined\n",
    "- not necessarily continuous if out not defined as all possible real number values\n",
    "    - then would be defined as a large discete set\n",
    "- one the other hand, numbers do have an ordinal meaning (15 yrs > 14 yrs)\n",
    "\n",
    "## Key terms in classification learning\n",
    "- **Instances (or rows)**\n",
    "    - Set of inputs / features\n",
    "    - Vectors of values that define the input space\n",
    "- **Concept** (i.e. a function)\n",
    "    - _An idea that describes a set of things i.e. a set of instances)_\n",
    "    - The function that maps values without the input space to values within the output space (i.e. assigning labels to a set of features)\n",
    "- **Target concept**\n",
    "    - \"The actual answer\"\n",
    "    - The particular concept / idea we are trying to find / represent.\n",
    "    - The concept (or concepts) that we are trying to single out from all possbile concepts that could map a specific set of inputs to outputs.\n",
    "- **Hypothesis class**\n",
    "    - Set of all concepts that we are willing to entertain\n",
    "    - Subset of all possible concepts that we restrict our search for the target concept\n",
    "- **Sample (or training set)**\n",
    "    - Set of example input-output pairs\n",
    "    - Explains the _target concept_\n",
    "- **Candidate**\n",
    "    - Concept that we think might be the target concept\n",
    "- **Testing set**\n",
    "    - Use this to determine whether the candidate concept is the target concept or not.\n",
    "    - Should be separate from training set, makes sure that a concept is generalizeable.\n",
    "\n",
    "## Decision Trees\n",
    "- Examples used by instructors are _binary classification_ problems\n",
    "\n",
    "**Representation vs. Algorithm**\n",
    "- A decision tree is a specific _representation_ of a concept\n",
    "- Algorithm that builds the decision tree comes after we understand the representation.\n",
    "\n",
    "### Parts of a decision tree\n",
    "- _Node_: attribute\n",
    "- _Edge_: value\n",
    "- _Node without child_: output\n",
    "![decision tree diagram](decision_trees_images/decision_tree_diagram.png)\n",
    "\n",
    "### Defining the algorithm\n",
    "- 20 questions example: _goal is to narrow possibilities_\n",
    "    - Next question should _further_ narrow the possiblities, i.e. take into account previous questions and answers.\n",
    "    - Questions will start more general and gradually become more specific.\n",
    "    - Differs from algorithm in that 20 questions does not have a defined input space.\n",
    "\n",
    "**Steps in building decision tree algorithm**\n",
    "1. Pick \"best\" attribute to query, one that splits data roughly in half\n",
    "- Ask question based on attribute.\n",
    "- Follow path of the answer\n",
    "- Go to step 1 if no final answer, else return answer.\n",
    "\n",
    "#### Quiz: Determining the \"best\" attribute\n",
    "_(rank which attribute is best; 1 is best, 3 is worst)_\n",
    "- Note: Attr ranked 2 might be worse than one ranked 3\n",
    "    - Middle attr does not discriminate different labels.\n",
    "    - Left attr does but in no meaningful way, and having this attribute would contribute to over-fitting since it trains the algorithm without any generalization value.\n",
    "\n",
    "![quiz best attribute](decision_trees_images/quiz_best_attribute.png)\n",
    "\n",
    "### DTs Expressiveness\n",
    "(moving to less abstract description)\n",
    "#### Boolean functions\n",
    "**b AND a** == a AND b\n",
    "- communicative as opposed to associative, so DTs are interchangeable\n",
    "![DT AND](decision_trees_images/dt_and.png)\n",
    "\n",
    "**b OR a** == a OR b:\n",
    "![DT OR](decision_trees_images/dt_or.png)\n",
    "\n",
    "**b XOR a** == a XOR b:\n",
    "- XOR a combination of OR and AND\n",
    "- Equivalent to \"either... or\" question (when having both things is not an option)\n",
    "- Tree for XOR is just another representation of the full truth table.\n",
    "![DT XOR](decision_trees_images/dt_xor.png)\n",
    "\n",
    "#### Generalizing boolean functions\n",
    "- for n-XORs, can solve with _odd partiy_ i.e. if odd number of Ts, result is True and vice verse for F\n",
    "    - Algorithm is exponential, classified as a _hard_ problem\n",
    "    - \"big O 2 to the N\" in terms of complexity / performance of alg\n",
    "- For ML algs to work, we need more easier problems like \"any\" questions (i.e. n-ORs). We do this by:\n",
    "    - finding an easier problem (for example, than an n-XOR one)\n",
    "    - or finding a clever shortcut like the sum equation in diagram below.\n",
    "\n",
    "How many nodes in terms of n-number of ORs and XORs?\n",
    "![DT n-ORs and n-XORs](decision_trees_images/dt_nors_nxors.png)\n",
    "\n",
    "#### How expressive are decision trees?\n",
    "- i.e. how many DTs are in the set of all possible DTs?\n",
    "    - (Need some combinatorics to solve)\n",
    "    - \"big O n-factorial\" for attributes / nodes\n",
    "    - using truth table to help generalize, there will be 2^N possible rows / instances for N number of columns \n",
    "    - Possible output variations will also be exponential at 2^N\n",
    "    - And so possible DTs is _2^(2^N)_ (a double exponential)\n",
    "- Therefore, the _hypothesis space_ that we have chosen (all possible DTs for n attributes) is very expressive.\n",
    "    - We need an algorithm to create a subset of all possible DTs\n",
    "![DT expressivity](decision_trees_images/dt_expressivity.png)\n",
    "![DT possibilities](decision_trees_images/dt_possibilities.png)\n",
    "\n",
    "### ID3 Algorithm for Decision Trees\n",
    "\n",
    "#### Entropy\n",
    "__definition:__ a measure of impurity in a bunch of examples\n",
    "\n",
    "- 1.0 is evenly split classes in a node, where 0.0 is a node uniform with respect to a class\n",
    "- Controls how a DT decides where to split the data.\n",
    "- Other measures include \"gini\", the default criterion parameter for `sklearn.tree.DecisionTreeClassifier()`\n",
    "\n",
    "#### Information Gain\n",
    "- See past notes from Intro to Machine Learning course.\n",
    "\n",
    "#### ID3 Algorithm Details\n",
    "![ID3 algorithm sequence](decision_trees_images/id3_alg_sequence.png)\n",
    "\n",
    "\n",
    "#### ID3 Bias\n",
    "1. Restriction bias\n",
    "- Preference bias (at the heart of inductive bias)\n",
    "    - Bias toward better splits near the top of the tree\n",
    "    - Prefers trees that give accurate labels\n",
    "    - shorter trees to longer tree (comes naturally from preference for good trees at the top).\n",
    "\n",
    "#### Working with Continuous Attributes\n",
    "- DTs work well with discrete attributes\n",
    "- What about continuous attributes? (??not sure about this)\n",
    "    - Separate values into ranges, split on range would be True or False\n",
    "    - Would use training data to determine what ranges (using test data as well would cause contamination)\n",
    "    - Idea from instructor: Could use _binary search_ method, starting with a range at the root that splits the range of values for that attribute in the training set, and continues halving ranging down the tree (at least for that particular attribute, because other attributes may provide more information gain).\n",
    "- Quiz: Does it make sense to repeat an attribute along a path in the tree?\n",
    "    - For discrete values, _no_. Would not provide any information gain that wasn't achieve earlier in the tree.\n",
    "    - For continuous values, _yes_. Because range outside of previous ones could provide some information gain.\n",
    "\n",
    "#### When to Terminate ID3 Algorithm\n",
    "- _Not_ when everything is classified correctly because there will be noise in the training set. (Overfitting)\n",
    "- _Not_ necessarily when there are no more attributes to split on because continuous values can be infinitely split\n",
    "\n",
    "To prevent overfitting, i.e. having a DT that is too complex\n",
    "- Use cross-validation on each possible depth of DT (see instructor's for [6-min intro video](https://classroom.udacity.com/courses/ud675/lessons/312357973/concepts/4381086450923) for context)\n",
    "- _Pruning_ more efficient method, simple addition to ID3:\n",
    "    - Create DT to full depth\n",
    "    - Fold bottom nodes upward, checking error on validation set as it prunes.\n",
    "    \n",
    "#### DTs with Regression (briefly covered here)\n",
    "- Applying DTs with continuous functions.\n",
    "- With potentially _continuous output values_, or mix between the two.\n",
    "- Problem with Information Gain calculation\n",
    "- Need to look at criteria for splitting and nodes\n",
    "    - Could assess output values with \"voting\", with averages or Local Linear Fit (??)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
