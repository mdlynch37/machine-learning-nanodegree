{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Side notes \n",
    "_(code snippets, summaries, resources, etc.)_\n",
    "- SEE: Other books or textual material on regression calculation to revise and make notes missing below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression & Classification\n",
    "\n",
    "## Summary of topics covered\n",
    "![topics covered](regression_and_classification_images/topics_covered.png)\n",
    "\n",
    "\n",
    "\n",
    "## Regression vs. Classification\n",
    "__Regression:__ Using a functional form to approximate a set of data points.\n",
    "- Used for functions (mathematical relationships) that map _continuous inputs_ to discrete and/or continuous outputs\n",
    "\n",
    "__derived from:__ Regress to the mean\n",
    "- example of tall parent's child:\n",
    "    - the average height of children like her will be between parent's height and average height\n",
    "    - average height can be seen as that of the tall parent's ancestors, which will factor into child's height\n",
    "    - thus slope of line mapping data points below will be less than 1.\n",
    "\n",
    "![Regression to the mean](regression_and_classification_images/regress_to_mean.png)\n",
    "\n",
    "## Regression in Machine Learning\n",
    "### Calculating best constant function\n",
    "- Use calculus to minimize error function for `y = c`.\n",
    "- Least squares method for determing best function is suited for calculus, results include mean value.\n",
    "- Final result for line of least squares for `y = c` is the average y values for data points.\n",
    "\n",
    "![Best constant function](regression_and_classification_images/best_constant_function.png)\n",
    "\n",
    "### Order of polynomial\n",
    "SEE: Other books or textual material on regression calculation to revise and augment notes (some material not included in this notebook because it is too time-consuming to take notes from videos).\n",
    "\n",
    "#### Higher degrees of freedom -> lower training error\n",
    "\n",
    "![least squared error polynomials](regression_and_classification_images/least_squared_error_polynomials.png)\n",
    "\n",
    "![degrees of freedom and error](regression_and_classification_images/degrees_of_freedom_and_error.png)\n",
    "\n",
    "### Polynomial Regression\n",
    "- Using linear algebra and projections to solve (calculus provides some depth that will be covered later)\n",
    "\n",
    "![polynomial regression 1](regression_and_classification_images/polynomial_regression_1.png)\n",
    "\n",
    "![polynomial regression 2](regression_and_classification_images/polynomial_regression_2.png)\n",
    "\n",
    "### Errors\n",
    "- Regression does not model `f` _but rather `f + e`_ (training data has errors)\n",
    "- Where do these errors come from?\n",
    "    - sensor error / noise\n",
    "    - unmodeled influences, noise that has a \"bumpy\" or inconsistent influence across the data\n",
    "    - being given bad data, sometimes purposefully or maliciously\n",
    "    - transcription error, copying mistakes\n",
    "    \n",
    "### Cross Validation\n",
    "- Training and testing data are assumed to be representative of unknown data in the future.\n",
    "- Fundamental assumption is statistics:\n",
    "    - Data is Independant and Identically Distributed (IID)\n",
    "- Cross validation set is a \"test\" set within the training set, a \"trial test set\"\n",
    "    - Used to balance model's complexity so that it is generalizable enough to perform well on the test set.\n",
    "    - k-fold method outlined, see: notes elsewhere\n",
    "\n",
    "__Beauty of CV methods__\n",
    "(still using housing prices example)\n",
    "- Maximizes complexity without overfitting, i.e. _fitting the model to the error_\n",
    "\n",
    "![CV example 1](regression_and_classification_images/cross_validation_example_1.png)\n",
    "![CV example 2](regression_and_classification_images/cross_validation_example_2.png)\n",
    "\n",
    "### Other Input Spaces\n",
    "- Concept of linear and polynomial functions generalizes to n-dimensions for n-number of input variables\n",
    "- Inputs that can be fed into a regression model:\n",
    "    - scalar, continuous\n",
    "    - vector, continuous\n",
    "    - discrete, vector or scalar\n",
    "    - categories / types\n",
    "        - each type converted into its own boolean variable\n",
    "        - if converted into numerical codes, regression will interpret one type as being close to another type (not the case)\n",
    "- Will cover these aspect more in depth when discussing converting questions into ML problems\n",
    "\n",
    "![other input spaces](regression_and_classification_images/other_input_spaces.png)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
