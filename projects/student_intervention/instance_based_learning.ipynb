{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Side notes \n",
    "_(code snippets, summaries, resources, etc.)_\n",
    "- Provides extra explanation: [_Instance Based Learning Extension_ PDF by Udactiy](https://www.evernote.com/shard/s37/nl/1033921335/daef3986-2f0a-4a28-bab2-fd8f68178549/) (Evernote)\n",
    "- Wikipedia: [k-nearest neighbors algorithm](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)\n",
    "- Learn more about the Manhattan metric (or Taxicab distance) in the Wikipedia article: [Taxicab geometry](http://en.wikipedia.org/wiki/Taxicab_geometry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instance Based Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of topics covered\n",
    "![summary of instance based learning](instance_based_learning_images/summary_instance_based_learning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Pros and Cons of Instance Based Learning__\n",
    "\n",
    "![instance based learning overview](instance_based_learning_images/instance_based_learning_overview.png)\n",
    "\n",
    "\n",
    "__k-NN with housing prices__\n",
    "\n",
    "![k-NN with housing prices](instance_based_learning_images/k-nn_housing_prices.png)\n",
    "\n",
    "__k-NN Algorithm__\n",
    "- Two parameters for algorithm: \n",
    "    1. k in k-NN\n",
    "    - distance metric (stand-in for similarity, more generalized idea for different criterion for measuring distance)\n",
    "- Simple algorithm but lots of decision points / tweaks for implementation\n",
    "    \n",
    "![k-NN algorithm](instance_based_learning_images/k-nn_algorithm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: Wont You Compute My Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- _Lazy_ learners like k-NN procrastinate learning until absolutley necessary during query time, \"just in time learning\"\n",
    "\n",
    "![k-NN performance quiz](instance_based_learning_images/k-nn_performance_quiz.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: Domain K NNowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Shows that k-NN does not do very well in the example, because example fits into type of data set where k-nn's preference bias really comes out.\n",
    "- But shows how different parameters for distance metric and _k_ have large impact on output\n",
    "\n",
    "![calculating k-NN quiz](instance_based_learning_images/calculating_k-nn_quiz.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-NN Preference Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![k-NN preference bias](instance_based_learning_images/k-nn_bias.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curse of Dimensionality\n",
    "(copied from P1: Causes of Error jupn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Rule:__ As the number of features or dimensions grows, the amount of data we need to generalize accurately grows exponentially.\n",
    "- Very related to __feature selection__\n",
    "\n",
    "(don't quite understand example)\n",
    "- \"We are talking about data points in a k nearest neighbor method...\"\n",
    "- \"Isn't just an issue for KNN, true in general... Think of it as points that are representing or covering the space... And coverage is necessary for learning. Applies to all of ML.\"\n",
    "\n",
    "![curse of dimensionality](instance_based_learning_images/curse_of_dimensionality.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-NN parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The similarity function and k-value are ways of capturing domain knowledge._\n",
    "- Domain knowledge matters!\n",
    "\n",
    "__Similarity / distance function:__\n",
    "- Distance function selection is very important\n",
    "    - Can use weighted combination of different functions (like with y = x1^2 * x2 example that combined Euclidean and Manhattan functions to give x1 more impact on output).\n",
    "    - This is a workaround to preference bias of equally weighted features.\n",
    "- Distance function can broadly encompass any expressed similarity based on domain knowledge (what we know about relationships in our data set).\n",
    "- Distance function can be replaced by other learners like DTs, neural networks, linear regressions (called _locally weighted linear regression_), etc.\n",
    "- These locally weighted learners allow expansion of hypothesis space from lines to more than lines.\n",
    "- k-nn is powerful because it provides for arbitrarily complicated functions based on data points similar to the query point.\n",
    "    - \"locally smooth, but globally bumpy\"\n",
    "\n",
    "__`k`:__\n",
    "- if `k = n`, algorithm computes weighted averages of all data points\n",
    "- output will be based on location of query point even though this will not determine which neighbors are considered.\n",
    "- This aspect is what separately k-nn from a linear regression which gives all points equal weight. (See graph below with different \"lines\" for different queries.)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
