{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sidenotes (definitions, code snippets, resources, etc.)\n",
    "- Original in `nd_machine_learning/nd_ml_course_code/projects/boston_housing/`\n",
    "- Symlinked to `intro_to_ml/ud120-projects/evaluation/` for lesson 14 in Intro to ML course.\n",
    "\n",
    "### Python\n",
    "- Note on data structure: list\n",
    "    - empty list has a truth value of false\n",
    "    \n",
    "### Further Reading\n",
    "- Definition of [Coefficient of Determination](http://stattrek.com/statistics/dictionary.aspx?definition=coefficient_of_determination) in Stat Trek's Statistics and Probability Dictionary\n",
    "- Investigate meaning of `# %%writefile new_enron_feature.py` inserted at top of edited studentMain.py module\n",
    "\n",
    "### Latex\n",
    "To use Python to display Latex equations etc.:\n",
    "```python\n",
    "from IPython.display import display, Math, Latex\n",
    "display(Math(\n",
    "    r'F(k) = \\int_{-\\infty}^{\\infty} f(x) e^{2\\pi i k} dx'))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification vs Regression Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In classification we want to see how often a model correctly or incorrectly identifies a new example, whereas in regression we might be more interested to see how far off the model's prediction is from the real true value.\n",
    "- _Classification metrics_ include: accuracy, precision, recall, and F-score.\n",
    "    - Using a set of data kept for testing, we can use these metrics on this testing set to measure which points were accurately classified, and which were not.\n",
    "- _Regression metrics_ include: mean absolute error and mean squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__definition:__ \n",
    "\n",
    "$\\text{accuracy} = \\frac{\\text{no. of data points labeled corrected}}{ \\text{all data points}} = \\frac{\\text{true positives + true negatives}}{ \\text{all data points}}$\n",
    "- Accuracy here is described as the proportion of items classified or labeled correctly.\n",
    "- Most basic and common classification metric\n",
    "- Default metric used in the `clf.score()` method in sklearn (\n",
    "- Can also use `sklearn.metrics.`[__`accuracy_score()`__](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score)\n",
    "\n",
    "#### Shortcoming of accuracy measurement:\n",
    "- Not good for skewed classes (i.e. most of the data under one label)\n",
    "    - because demoninator will be small, so measurement not trustworthy\n",
    "- Not suited to particular labeling requirements i.e. might want the metric to err on one label over the other.\n",
    "    - different performance metrics can focus on different types of errors (false positives, false negatives).\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrices: understanding precision and recall "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mathematical representation of classification errors by type\n",
    "- Example of Confusion Matrix analysis with Decision Tree:\n",
    "![confusion matrix example](evaluation_images/confusion_matrix_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{Precision(x)} = \\frac{\\text{data points correctly labeled as x}}{ \\text{total data points predicted x}} = \\frac{\\text{true positives}}{ \\text{true positives + false positives}}$\n",
    "- in sklearn: `sklearn.metrics.`[__`precision_score()`__](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html)\n",
    "- Measures confidence in model's positive predictions for a specific label.\n",
    "    - i.e. a high precision relates to a low false positive rate\n",
    "- Gives the probability that a postive prediction of label x for a test data point is accurate.\n",
    "- Answers q: of all the predicted positives, how many are accurate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{Recall(x)} = \\frac{\\text{data points correctly labeled as x}}{ \\text{total data points actually x}} = \\frac{\\text{true positives}}{ \\text{true positives + false negatives}}$\n",
    "- in sklearn: `sklearn.metrics.`[__`recall_score()`__](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html)\n",
    "- Measures confidence in model's negative predictions for a specific label.\n",
    "    - i.e. a high recall relates to a low false negative rate\n",
    "- Gives the probability that a negative prediction of label x for a test data point is accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$F_{1} = 2* \\frac{\\text{precision $\\cdot$ recall}}{\\text{precision + recall}}$\n",
    "- in sklearn: `sklearn.metrics.`[__`f1_score()`__](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)\n",
    "- Combines precision and recall relative to a specific positive class.\n",
    "- Can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst at 0.\n",
    "- The relative contribution of precision and recall to the F1 score are equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Regression scoring functions return values between 0 and 1, like classification metrics (better than error measurements in this way).\n",
    "    - Covered briefly for use in Boston Housing project\n",
    "- Regression error functions measure how close a regression model's prediction is to a true value. Lower value indicates better performance of a model in contrast to scoring functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Absolute Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in sklearn: `sklearn.metrics.`[__`mean_absolute_error()`__](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html)\n",
    "- The average of the distances from predicted values to their true values (a.ka. _residual errors_).\n",
    "- _Absolute_ error is used to avoid canceling out errors from being too high or below the true values (similar to problem seen when using different methods to measure variance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in sklearn: `sklearn.metrics.`[__`mean_squared_error()`__](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html)\n",
    "- Most common metric to measure model performance. \n",
    "- In contrast with absolute error, the each residual error is squared.\n",
    "- Advantages of squaring each error:\n",
    "    - Error terms are positive\n",
    "    - larger errors are emphasized over smaller errors\n",
    "    - equation is differentiable, allowing calculus calculation of minimum and maximum values, which often leads to better computational efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R2 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in sklearn: `sklearn.metrics.`[__`r2_score()`__](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html)\n",
    "- Default scoring method for regression learners in sklearn\n",
    "- Computes the _coefficient of determination_ of predictions for true values. This is the default scoring method for regression learners in scikit-learn\n",
    "\n",
    "**Definition of [Coefficient of Determination](http://stattrek.com/statistics/dictionary.aspx?definition=coefficient_of_determination):** (from Stat Trek)\n",
    "\n",
    "- The coefficient of determination (denoted by R<sup>2</sup>) is a key output of regression analysis. \n",
    "- It is interpreted as the proportion of the variance in the dependent variable that is predictable from the independent variable.\n",
    "    - The coefficient of determination is the square of the correlation (r) between predicted y scores and actual y scores; thus, it ranges from 0 to 1.\n",
    "    - With linear regression, the coefficient of determination is also equal to the square of the correlation between x and y scores.\n",
    "\n",
    "Meaning of return value:\n",
    "- A model with an R<sup>2</sup> of 0 always fails to predict the target variable, whereas a model with an R<sup>2</sup> of 1 perfectly predicts the target variable.\n",
    "- Any value between 0 and 1 indicates what percentage of the **target variable**, using this model, can be explained by the **features**.\n",
    "- *A model can be given a negative R<sup>2</sup> as well, which indicates that the model is no better than one that naively predicts the mean of the target variable.*\n",
    "\n",
    "Formula of the Coefficient of Determination from sklearn's [User Guide](http://scikit-learn.org/stable/modules/model_evaluation.html#r2-score)\n",
    "![formula of the coefficient of determination](evaluation_images/formula_coefficient_of_determination.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explained Variance Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in sklearn: `sklearn.metrics.`[__`explained_variance_score()`__](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.explained_variance_score.html)\n",
    "- Covered in more detail later in nanodegree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-project! Applying Metrics to Your POI Identifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[not labeled mini-project in course]\n",
    "\n",
    "Go back to your code from the last lesson, where you built a simple first iteration of a POI identifier using a decision tree and one feature. Copy the POI identifier that you built into the skeleton code in evaluation/evaluate_poi_identifier.py. Recall that at the end of that project, your identifier had an accuracy (on the test set) of 0.724. Not too bad, right? Let’s dig into your predictions a little more carefully.\n",
    "\n",
    "From Python 3.3 forward, a change to the order in which dictionary keys are processed was made such that the orders are randomized each time the code is run. This will cause some compatibility problems with the graders and project code, which were run under Python 2.7. To correct for this, add the following argument to the featureFormat call on line 25 of evaluate_poi_identifier.py:\n",
    "\n",
    "sort_keys = '../tools/python2_lesson14_keys.pkl'\n",
    "\n",
    "This will open up a file in the tools folder with the Python 2 key order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "UDACITY_PATH = '~/projects/udacity/intro_to_ml/ud120-projects/evaluation/'\n",
    "if not UDACITY_PATH in sys.path:\n",
    "    sys.path.append(UDACITY_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named feature_format",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-6dc29e852b7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Final model from L13\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mevaluate_poi_identifier\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m### it's all yours from here forward!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mdlynch37/projects/udacity/nd_machine_learning/nd_ml_course_code/projects/boston_housing/evaluate_poi_identifier.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../tools/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfeature_format\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfeatureFormat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargetFeatureSplit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mdata_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../final_project/final_project_dataset.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named feature_format"
     ]
    }
   ],
   "source": [
    "# Final model from L13\n",
    "from evaluate_poi_identifier import *\n",
    "\n",
    "### it's all yours from here forward!\n",
    "from sklearn.cross_validation import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score    \n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(features_train, labels_train)\n",
    "pred = clf.predict(features_test)\n",
    "print \"Accuracy score: \", accuracy_score(labels_test, pred)\n",
    "print \"No. POIs predicted in test set: \", len([x for x in pred if x == 1])\n",
    "print \"No. of true positives: \", (\n",
    "    len([i for i, j in zip(labels_test, pred) if i and j == 1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may now see, having imbalanced classes like we have in the Enron dataset (many more non-POIs than POIs) introduces some special challenges, namely that you can just guess the more common class label for every point, not a very insightful strategy, and still get pretty good accuracy!\n",
    "\n",
    "Precision and recall can help illuminate your performance better. Use the precision_score and recall_score available in sklearn.metrics to compute those quantities.\n",
    "\n",
    "What’s the precision?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "print \"Precision score: \", precision_score(labels_test, pred)\n",
    "print \"Recall score: \", recall_score(labels_test, pred)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py2]",
   "language": "python",
   "name": "conda-env-py2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
