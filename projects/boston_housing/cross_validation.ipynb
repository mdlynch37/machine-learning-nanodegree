{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sidenotes (definitions, code snippets, resources, etc.)\n",
    "- Original in `nd_machine_learning/nd_ml_course_code/projects/boston_housing/`\n",
    "- Symlinked to `intro_to_ml/ud120-projects/validation/` for lesson 13 in Intro to ML course.\n",
    "\n",
    "### ML Order of Operations\n",
    "![order of operations](cross_validation_images/ml_order_of_operations.png)\n",
    "\n",
    "### Python 3 change\n",
    "- From Python 3.3, dict keys are iterating through in a random order for each iteration (will alter GridSearchCV's output).\n",
    "    - See note with validation mini-project for info on coverting code from 2.7 to 3.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation\n",
    "sklearn User Guide [3.1. Cross-validation: evaluating estimator performance](http://scikit-learn.org/stable/modules/cross_validation.html):\n",
    "- When evaluating different settings (“hyperparameters”) for estimators, such as the `C` setting that must be manually set for an SVM, there is still a risk of overfitting _on the test set_ because the parameters can be tweaked until the estimator performs optimally. \n",
    "- This way, knowledge about the test set can “leak” into the model and evaluation metrics no longer report on generalization performance. \n",
    "- To solve this problem, yet another part of the dataset can be held out as a so-called “validation set”: training proceeds on the training set, after which evaluation is done on the validation set, and when the experiment seems to be successful, final evaluation can be done on the test set.\n",
    "- However, by partitioning the available data into three sets, we drastically reduce the number of samples which can be used for learning the model, and the results can depend on a particular random choice for the pair of (train, validation) sets.\n",
    "- A solution to this problem is a procedure called cross-validation (CV for short). A test set should still be held out for final evaluation, but the validation set is no longer needed when doing CV. In the basic approach, called _k_-fold CV, the training set is split into _k_ smaller sets (other approaches are described below, but generally follow the same principles). The following procedure is followed for each of the _k_ “folds”:\n",
    "    - A model is trained using `k-1` of the folds as training data;\n",
    "    - the resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as accuracy).\n",
    "    - The performance measure reported by _k_-fold cross-validation is then the average of the values computed in the loop. \n",
    "- This approach can be computationally expensive, but does not waste too much data (as it is the case when fixing an arbitrary test set), which is a major advantage in problem such as inverse inference where the number of samples is very small.\n",
    "\n",
    "## KFold in sklearn\n",
    "- in sklearn: `sklearn.cross_validation.`[__`KFold()`__](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.KFold.html)\n",
    "- Does not randomize data automatically (can cause issues with performance)\n",
    "- Use keyword argument `shuffle=True` to randomized events.\n",
    "\n",
    "Example usage from [Cross-validation on diabetes Dataset Exercise](http://scikit-learn.org/stable/auto_examples/exercises/plot_cv_diabetes.html):\n",
    "```python\n",
    "lasso_cv = linear_model.LassoCV(alphas=alphas)\n",
    "k_fold = cross_validation.KFold(len(X), 3)\n",
    "\n",
    "...\n",
    "\n",
    "for k, (train, test) in enumerate(k_fold):\n",
    "    lasso_cv.fit(X[train], y[train])\n",
    "    print(\"[fold {0}] alpha: {1:.5f}, score: {2:.5f}\".\n",
    "          format(k, lasso_cv.alpha_,\n",
    "                 lasso_cv.score(X[test],\n",
    "                 y[test])))\n",
    "```\n",
    "\n",
    "## GridSearchCV in sklearn\n",
    "`sklearn.grid_search`.GridSearchCV [Documentation](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html) and [User Guide](http://scikit-learn.org/stable/modules/grid_search.html#grid-search):\n",
    "- Parameters that are not directly learnt within estimators can be set by searching a parameter space for the best performance score. \n",
    "- Typical examples include `C`, `kernel` and `gamma` for Support Vector Classifier, `alph` for Lasso, etc.\n",
    "- Parameters passed to GridSearchCV in a _parameter space_ are often referred to as _hyperparameters_ (particularly in Bayesian learning), distinguishing them from the parameters optimised in a machine learning procedure.\n",
    "\n",
    "Example from documentation, explained:\n",
    "```python\n",
    "from sklearn import svm, grid_search, datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "svr = svm.SVC()\n",
    "parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    "clf = grid_search.GridSearchCV(svr, parameters)\n",
    "clf.fit(iris.data, iris.target)\n",
    "```\n",
    "- `parameters` is a dict of different sets of parameters that will be used to train multiple SVM classifiers.\n",
    "- `svr = svm.SVC()` is passed to the GridSearchCV classifier to indicate what classifier iterate.\n",
    "- `clf = grid_search.GridSearchCV(svr, parameters)` creates the classifier by generating a 'grid' of SMVs from each of the given combinations of values for (kernel, C).\n",
    "- `clf.fit(iris.data, iris.target)` iterates through the grid, returning a fitted classifier automatically tuned to the optimal parameter combination. \n",
    "    - `clf.best_params_` returns those parameter values.\n",
    "    - `grid.best_estimator_` returns the optimized estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to the eigenfaces code, which you can find here. What parameters of the SVM are being tuned with GridSearchCV?\n",
    "\n",
    "- _Answer:_ 5 values of C and 6 values of gamma are tested out.\n",
    "\n",
    "## Mini-project! on validation\n",
    "You’ll start by building the simplest imaginable (unvalidated) POI identifier. The starter code (validation/validate_poi.py) for this lesson is pretty bare--all it does is read in the data, and format it into lists of labels and features. Create a decision tree classifier (just use the default parameters), train it on all the data (you will fix this in the next part!), and print out the accuracy. THIS IS AN OVERFIT TREE, DO NOT TRUST THIS NUMBER! Nonetheless, what’s the accuracy?\n",
    "\n",
    "- _Answer:_ 0.98947368421052628. \n",
    "    - \"Pretty high accuracy, huh?  Yet another case where testing on the training data would make you think you were doing amazingly well, but as you already know, that's exactly what holdout test data is for...\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98947368421052628"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from validate_poi import *\n",
    "\n",
    "### it's all yours from here forward! \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(features, labels)\n",
    "pred = clf.predict(features)\n",
    "accuracy_score(pred, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you’ll add in training and testing, so that you get a trustworthy accuracy number. Use the train_test_split validation available in sklearn.cross_validation; \n",
    "- hold out 30% of the data for testing and \n",
    "- set the random_state parameter to 42 (random_state controls which points go into the training set and which are used for testing; setting it to 42 means we know exactly which events are in which set, and can check the results you get). \n",
    "\n",
    "What’s your updated accuracy?\n",
    "\n",
    "- _Answer:_ 0.72413793103448276\n",
    "    - Properly deployed with \"testing data brings us back down to earth after that 99% accuracy in the last quiz.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72413793103448276"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from validate_poi import *\n",
    "\n",
    "### it's all yours from here forward! \n",
    "from sklearn.cross_validation import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score    \n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(features_train, labels_train)\n",
    "pred = clf.predict(features_test)\n",
    "accuracy_score(pred, labels_test)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
