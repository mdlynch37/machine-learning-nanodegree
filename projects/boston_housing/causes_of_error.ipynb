{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sidenotes\n",
    "- Includes feature selection notes from lesson 11 Intro to Machine Learning, and other notes from p1 course material.\n",
    "\n",
    "### Important Reading\n",
    "- [Understanding the Bias-Variance Tradeoff](http://scott.fortmann-roe.com/docs/BiasVariance.html) by Scott Fortmann-Roe.\n",
    "\n",
    "__Learning curve quiz complete but not understood__, read over:\n",
    "- `sklearn.learning_curve`.[__`learning_curve()`__](http://scikit-learn.org/stable/modules/generated/sklearn.learning_curve.learning_curve.html)\n",
    "- Sklearn Example: [Plotting Learning Curves](http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html#example-model-selection-plot-learning-curve-py)\n",
    "- [User Guide](http://scikit-learn.org/stable/modules/learning_curve.html#learning-curves) 3.5. Validation curves: plotting scores to evaluate models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "1. Bias & Variance\n",
    "- Learning Curves & Model Complexity\n",
    "- Representative Power of a Model (Curse of Dimensionality)\n",
    "- Feature Selection\n",
    "    1. Univariate Feature Selection (`SelectPercentile`, `SelectKBest`)\n",
    "    - Feature Selection in TfIdf Vectorizer\n",
    "    - Regularization (Lasso Regression in sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias & Variance\n",
    "## Error due to Bias, i.e. underfitting\n",
    "High bias algorithm pays _too little_ attention to data, not effected much by data (i.e. oversimplified)\n",
    "- characterized by _high error_ on training set, i.e. low r<sup>2</sup> / large SSE or sum of squared (residual) errors\n",
    "- _common when:_ too few features used in model\n",
    "\n",
    "Bias measures a model's inability to represent the complexity of the underlying data, low accuracy score.\n",
    "- Common when there is enough data but not enough features, not rich enough\n",
    "- As a result, the model consistently and systematically misrepresents the data, leading to low accuracy in prediction, i.e. _underfitting_ to data.\n",
    "\n",
    "Simply put, __bias occurs when we have an inadequate model__, as in the following examples:\n",
    "- An example might be when we have objects that are classified by color and shape, for example easter eggs, but our model can only partition and classify objects by color. It would therefore consistently mislabel future objects--for example labeling rainbows as easter eggs because they are colorful.\n",
    "- Another example would be continuous data that is polynomial in nature, with a model that can only represent linear relationships. In this case it does not matter how much data we feed the model because it cannot represent the underlying relationship. To overcome error from bias, we need a more complex model.\n",
    "\n",
    "\n",
    "    \n",
    "## Error due to Variance, i.e. overfitting\n",
    "High variance algorithm pays _too much_ attention to data, does not generalize well, i.e. it overfits to the data\n",
    "- characterized by _much higher error_ on test set than training (some variance between the two is expected).\n",
    "- _common when:_ carefully minimized SSE, with too many features i.e. overfit to data\n",
    "\n",
    "Variance in this sense is a measure of how much the predictions vary for any given test sample.\n",
    "- A certain amount of variance is normal when training a model with randomly selected subsets of data\n",
    "- Too much variance indicates that the model is unable to generalize its predictions to the larger population, low precision score.\n",
    "- High sensitivity to the training set is also known as _overfitting_, and generally occurs when:\n",
    "    - the model is too complex\n",
    "    - there is not enough data to support it.\n",
    "- To reduce the variability of a model's predictions (and increase precision) by:\n",
    "    - training on more data\n",
    "    - limiting the model's complexity.\n",
    "\n",
    "## Trade-off: quality of model vs. no. of features\n",
    "To minimize bias and variance, we must find the right level of model complexity, balancing:\n",
    "- the performance/accuracy of the model on the training data\n",
    "    - (without overfitting)\n",
    "- with as few features as possible.\n",
    "    - (while maintaining algorithm's performance)\n",
    "    \n",
    "In addition to the subset of data chosen for training, what features you use from a given dataset can also greatly affect the bias and variance of your model.\n",
    "\n",
    "For more info, read [Understanding the Bias-Variance Tradeoff](http://scott.fortmann-roe.com/docs/BiasVariance.html) by Scott Fortmann-Roe.\n",
    "    \n",
    "## Visualizing Overfitting\n",
    "### An Overfit Regression\n",
    "- Blue points are training data\n",
    "- Red points are test data\n",
    "![An Overfit Regression](causes_of_error_images/overfit_regression.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Curves and Model Complexity\n",
    "## Learning Curves in Machine Learning\n",
    "- A graph that compares the performance of a model on training and testing data over a varying number of training instances.\n",
    "- The performance of a model should generally improve as the number of training points increases.\n",
    "- By separating training and testing sets and graphing performance on each separately, we can get a better idea of how well the model can generalize to unseen data.\n",
    "- A learning curve allows us to verify when a model has learned as much as it can about the data. When this occurs, the performance on both training and testing sets plateau and there is a consistent gap between the two error rates.\n",
    "\n",
    "### Bias\n",
    "- When the training and testing errors converge and are quite high this usually means the model is biased. \n",
    "- No matter how much data we feed it, the model cannot represent the underlying relationship and therefore has systematic high errors.\n",
    "\n",
    "### Variance\n",
    "- When there is a large gap between the training and testing error this generally means the model suffers from high variance. \n",
    "- Unlike a biased model, models that suffer from variance generally require more data to improve. \n",
    "- We can also limit variance by simplifying the model to represent only the most important features of the data.\n",
    "\n",
    "### Ideal Learning Curve\n",
    "- The ultimate goal for a model is one that has good performance that generalizes well to unseen data. \n",
    "- In this case, both the testing and training curves converge at similar values. \n",
    "    - The smaller the gap between the training and testing sets, the better our model generalizes. \n",
    "    - The better the performance on the testing set, the better our model performs.\n",
    "    \n",
    "## Model Complexity\n",
    "Validation curves in sklearn:\n",
    "- `sklearn.learning_curve.`[__`validation_curve()`__](http://scikit-learn.org/stable/modules/generated/sklearn.learning_curve.validation_curve.html) documentation\n",
    "- [User Guide](http://scikit-learn.org/stable/modules/learning_curve.html#validation-curve): 3.5.1. Validation curve\n",
    "\n",
    "The visual technique of graphing performance is not limited to learning. With most models, we can change the complexity by changing the inputs or parameters.\n",
    "\n",
    "A model complexity graph looks at training and testing curves as the model's complexity varies. __The most common trend is that as a model's complexity increases, bias will fall off and variance will rise.__\n",
    "\n",
    "Sklearn provides a tool for validation curves which can be used to monitor model complexity by varying the parameters of a model. We'll explore the specifics of how these parameters affect complexity in the next course on supervised learning.\n",
    "\n",
    "## Learning Curves and Model Complexity\n",
    "So what is the relationship between learning curves and model complexity?\n",
    "\n",
    "If we were to take the learning curves of the same machine learning algorithm with the same fixed set of data, but create several graphs at different levels of model complexity, all the learning curve graphs would fit together into a 3D model complexity graph.\n",
    "\n",
    "If we took the final testing and training errors for each model complexity and visualized them along the complexity of the model we would be able to see how well the model performs as the model complexity increases.\n",
    "\n",
    "## Practical use of Model Complexity\n",
    "Knowing that we can identify issues with bias and variance by analyzing a model complexity graph, we now have a visual tool to help identify ways to optimize our models.\n",
    "\n",
    "This will be one of the core tools we use in the upcoming project.\n",
    "\n",
    "In the final section, we will introduce cross validation and grid search, which will give us a concrete, systematic way of searching through different levels of complexity to find the optimal model that complexity and learning curves give us a holistic understanding of."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz: Noisy Data, Complex Model\n",
    "_(correct, maybe.. but not understood)_\n",
    "\n",
    "- Graph plotted changes with each run. (Not sure exactly what the graph means, and what it's varying plots mean.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In this exercise we'll examine a learner which has high variance,\n",
    "# and tries to learn nonexistant patterns in the data.\n",
    "# Use the learning curve function from sklearn.learning_curve \n",
    "# to plot learning curves of both training and testing error. \n",
    "# Use plt.plot() within the plot_curve function\n",
    "# to create line graphs of the values.\n",
    "\n",
    "# parts of code taken from plot_learning_curve.py sklearn example\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.learning_curve import learning_curve\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import explained_variance_score, make_scorer\n",
    "import numpy as np\n",
    "\n",
    "size = 1000\n",
    "cv = KFold(size,shuffle=True)\n",
    "score = make_scorer(explained_variance_score)\n",
    "\n",
    "X = np.round(np.reshape(np.random.normal(scale=5,size=2*size),(-1,2)),2)\n",
    "y = np.array([[np.sin(x[0]+np.sin(x[1]))] for x in X])\n",
    "\n",
    "def plot_curve():\n",
    "    # YOUR CODE HERE\n",
    "    reg = DecisionTreeRegressor()\n",
    "    reg.fit(X,y)\n",
    "    print reg.score(X,y)\n",
    "\n",
    "    # TODO: Create the learning curve with the cv and score parameters defined above.\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        DecisionTreeRegressor(), X, y, cv=cv, scoring=score)\n",
    "    \n",
    "    # TODO: Plot the training and testing curves.\n",
    "    plt.figure()\n",
    "    # plt.title(title)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Testing score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    \n",
    "\n",
    "    # Show the result, scaling the axis for visibility\n",
    "    plt.ylim(-.1,1.1)\n",
    "    plt.show()\n",
    "    \n",
    "plot_curve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representative Power of a Model\n",
    "## Curse of Dimensionality\n",
    "__Rule:__ As the number of features or dimensions grows, the amount of data we need to generalize accurately grows exponentially.\n",
    "- Very related to __feature selection__\n",
    "\n",
    "(don't quite understand example)\n",
    "- \"We are talking about data points in a k nearest neighbor method...\"\n",
    "- \"Isn't just an issue for KNN, true in general... Think of it as points that are representing or covering the space... And coverage is necessary for learning. Applies to all of ML.\"\n",
    "\n",
    "![curse of dimensionality](causes_of_error_images/curse_of_dimensionality.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "## Why Feature Selection?\n",
    "\"Make everything as simple as possible, but no simpler\" - Albert Einstein\n",
    "\n",
    "Two major things aspects:\n",
    "1. Select best features, leaving out unecessary data\n",
    "- Adding new features to explore data, using intuition\n",
    "\n",
    "## Ignoring features\n",
    "### Reasons to ignore a feature\n",
    "- Too much noise, hard to dinstinguish whether it is reliably measuring what you want it to be measuring, i.e. data is not accurate/reliable enough.\n",
    "- Causes overfitting for some reason\n",
    "- Highly correlated with/strong related to a feature that is already present, breaking the model because mathematics stops working.\n",
    "- Unecessarily slows down training/testing process when feature is clearly not useful.\n",
    "\n",
    "## Features != Information\n",
    "__definition:__ Features vs. Information\n",
    "- A feature is a characteristic of particular data point that is attempting to _access_ information\n",
    "    - In general, we want bare minimum number of features that give as much information as possible\n",
    "- Can think of features as quantity vs. information as quality.\n",
    "\n",
    "## Feature selection tools in sklearn\n",
    "- Feature reduction a.k.a dimensionality reduction\n",
    "- Very important to be skeptical of features, esp. with high _dimensionality data_\n",
    "- In example in tools/email_preprocess.py described below, 90% of features we ignored with:\n",
    "    - insignificant impact on the classifier's accuracy, and \n",
    "    - performance improved in terms of the _time complexity_ of the classifier algorithm.\n",
    "\n",
    "### Univariate Feature Selection\n",
    "There are several go-to methods of automatically selecting your features in sklearn. Many of them fall under the umbrella of univariate feature selection, which treats each feature independently and asks how much _power_ it gives you in classifying or regressing.\n",
    "\n",
    "There are two big univariate feature selection tools in sklearn:\n",
    "- `SelectPercentile` and `SelectKBest`. \n",
    "- The difference is pretty apparent by the names:\n",
    "    - SelectPercentile selects the X% of features that are most powerful (where X is a parameter)\n",
    "    - SelectKBest selects K number of features that are most powerful (where K is a parameter).\n",
    "\n",
    "A clear candidate for feature reduction is text learning, since _the data has such high dimension_. We actually did feature selection in the Sara/Chris email classification problem during the first few mini-projects; you can see it in the code in tools/email_preprocess.py:\n",
    "```python\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "...\n",
    "...\n",
    "### feature selection, because text is super high dimensional and \n",
    "### can be really computationally chewy as a result\n",
    "selector = SelectPercentile(f_classif, percentile=10)\n",
    "selector.fit(features_train_transformed, labels_train)\n",
    "features_train_transformed = selector.transform(\n",
    "    features_train_transformed).toarray()\n",
    "features_test_transformed  = selector.transform(\n",
    "    features_test_transformed).toarray()\n",
    "```\n",
    "\n",
    "### Feature Selection in TfIdf Vectorizer\n",
    "- NOTE: Usually not a good idea to mix univariate feature selection and Feature Selection with TfIdf Vectorizer parameter `max_df`\n",
    "\n",
    "Example from tools/email_preprocess.py:\n",
    "```python\n",
    "### text vectorization--go from strings to lists of numbers\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                               stop_words='english')\n",
    "features_train_transformed = vectorizer.fit_transform(features_train)\n",
    "features_test_transformed  = vectorizer.transform(features_test)\n",
    "```\n",
    "\"`max_df=0.5`\" parameter means that words with a document frequency of more than 0.5 will be removed.\n",
    "- i.e. words that occur in more than 50% of the documents are not included as features\n",
    "- used because words that are probably too common to be very 'powerful', does not provide 'access' to information.\n",
    "\n",
    "\n",
    "### Regularization\n",
    "__definition:__ automatically penalizing extra features in model\n",
    "\n",
    "![Regularization graph](causes_of_error_images/regularization_graph.png)\n",
    "\n",
    "#### Lasso Regression (type of regularized regression)\n",
    "`sklearn.linear_model`.Lasso [Documentation][lasso_doc] and [User Guide][lasso_user]\n",
    "- Mathematical optimization of the bias-variance trade-off\n",
    "- Minimizes SSE like basic regression\n",
    "- But also minimizes term (penalty parameter * coefficients of regression)\n",
    "    - coefficients of regression describe/are related to no. of features\n",
    "    - if feature does not add enough precision, it's coefficient is set to 0\n",
    "        - simpler algorithm with this method since it can run through the formula in-place.\n",
    "- This means that any loss from an additional feature must be offset by the gain in precision by adding that feature.\n",
    "\n",
    "   \n",
    "![Lasso regression formula](causes_of_error_images/lasso_regression_formula.png)\n",
    "![Lasso regression penalty method](causes_of_error_images/lasso_regression_penalty_method.png)\n",
    "\n",
    "[lasso_doc]: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html\n",
    "\n",
    "[lasso_user]: http://scikit-learn.org/stable/modules/linear_model.html#lasso"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
