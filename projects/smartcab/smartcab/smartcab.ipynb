{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P4: Smartcab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement a Basic Driving Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, your only task is to get the **smartcab** to move around in the environment. At this point, you will not be concerned with any sort of optimal driving policy. Note that the driving agent is given the following information at each intersection:\n",
    "- The next waypoint location relative to its current location and heading.\n",
    "- The state of the traffic light at the intersection and the presence of oncoming vehicles from other directions.\n",
    "- The current time left from the allotted deadline.\n",
    "\n",
    "To complete this task, simply have your driving agent choose a random action from the set of possible actions `(None, 'forward', 'left', 'right')` at each intersection, disregarding the input information above. Set the simulation deadline enforcement, `enforce_deadline` to `False` and observe how it performs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***QUESTION:***\n",
    "\n",
    "Observe what you see with the agent's behavior as it takes random actions. Does the **smartcab** eventually make it to the destination? Are there any other interesting observations to note?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***ANSWER:***\n",
    "\n",
    "The instructions for this question explicitly state that these random actions disregarding _the state of the traffic light_. Since the already-coded `Environment.act()` flags `move_okay` depending on what action the agent wants to take and the state of the traffic light, I would have to change this method or write a new one (which does not make sense) to follow these instructions fully.\n",
    "\n",
    "However, we can more easily disregard the other inputs assessed outside of `Environment.act()` and assign random actions to our agent. By doing this we can see that our agent does make it to the destination in most cases. In the other cases it runs up against the `hard_time_limit` set to `-100`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inform the Driving Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Instructions:***\n",
    "\n",
    "Now that your driving agent is capable of moving around in the environment, your next task is to identify a set of states that are appropriate for modeling the **smartcab** and environment. The main source of state variables are the current inputs at the intersection, but not all may require representation. You may choose to explicitly define states, or use some combination of inputs as an implicit state. At each time step, process the inputs and update the agent's current state using the `self.state` variable. Continue with the simulation deadline enforcement `enforce_deadline` being set to `False`, and observe how your driving agent now reports the change in state as the simulation progresses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Notes:***\n",
    "\n",
    "Rewards:\n",
    "- `+=10.0` for each successfully completed trip, determined by:\n",
    "    - ~~L1 distance to destination: int~~\n",
    "    - rather, `Environment.agent_state[LearningAgent]['location']`\n",
    "-  `+2.0` for each action it executes successfully that obeys traffic rules (a legal action), determined by:\n",
    "    - inputs from `Environment.sense()`\n",
    "-  `+0.0` for any null action, determined by:\n",
    "    - inputs from `Environment.sense()`\n",
    "-  `-0.5` for action not prescribed but legal, determined by:\n",
    "    - inputs from `Environment.sense()`\n",
    "    - `LearningAgent.next_waypoint`\n",
    "-  `-1.0` for an action that violates traffic rules (illegal action), determined by:\n",
    "    - inputs from `Environment.sense()`\n",
    "\n",
    "State description options:\n",
    "- L1 distance to destination: int\n",
    "- light: 'green', 'red'\n",
    "- oncoming: 'forward', 'left', 'right'\n",
    "- left: 'forward', 'left', 'right'\n",
    "- right: 'forward', 'left', 'right'\n",
    "- next_waypoint:  'forward', 'left', 'right'\n",
    "\n",
    "\n",
    "What info needs to be in a state?\n",
    "- Characteristics that would determine an action's reward taken from that state.\n",
    "- i.e. enough info in `s` to give correct output for $R(s)$, reward function.\n",
    "- __BUT__ agent still must _learn_ traffic rules on its own, without that being hard-coded into state (like first attempts with concept of legality).\n",
    "\n",
    "State definition:\n",
    "```python\n",
    "\n",
    "self.State = namedtuple('State', ['next_waypoint', 'light', \\\n",
    "                                  'oncoming', 'left', 'right'])\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "___First attempt(s):___\n",
    "\n",
    "```python\n",
    "self.ActionsLegality = \\\n",
    "    namedtuple('ActionsLegality',\n",
    "               ['no_action', 'forward',\n",
    "                'left', 'right'])\n",
    "self.State = namedtuple('State', \n",
    "                ['next_waypoint', 'actions_legality'])\n",
    "```\n",
    "---\n",
    "***More efficient state description is:***\n",
    "\n",
    "```python\n",
    "self.State = namedtuple('State', ['next_waypoint', 'okay_moves'])\n",
    "self.OKAY_MOVES = ('all', 'all but left', 'right on red', 'none')\n",
    "```\n",
    "States in total: 12\n",
    "\n",
    "---\n",
    "\n",
    "i.e. \n",
    "1. Green light - no oncoming traffic (all actions)\n",
    "- Green light - oncoming traffic (no left turn)\n",
    "- Red light - no oncoming traffic (right turn only)\n",
    "- Red light - oncoming traffic from left (no actions)\n",
    "\n",
    "```\n",
    "State(next_waypoint='left', actions_legality=ActionsLegality(no_action=True, forward=True, left=True, right=True))\n",
    "State(next_waypoint='left', actions_legality=ActionsLegality(no_action=True, forward=True, left=False, right=True))\n",
    "State(next_waypoint='left', actions_legality=ActionsLegality(no_action=True, forward=False, left=False, right=True))\n",
    "State(next_waypoint='left', actions_legality=ActionsLegality(no_action=True, forward=False, left=False, right=False))\n",
    "\n",
    "State(next_waypoint='right', actions_legality=ActionsLegality(no_action=True, forward=True, left=True, right=True))\n",
    "State(next_waypoint='right', actions_legality=ActionsLegality(no_action=True, forward=True, left=False, right=True))\n",
    "State(next_waypoint='right', actions_legality=ActionsLegality(no_action=True, forward=False, left=False, right=True))\n",
    "State(next_waypoint='right', actions_legality=ActionsLegality(no_action=True, forward=False, left=False, right=False))\n",
    "\n",
    "State(next_waypoint='forward', actions_legality=ActionsLegality(no_action=True, forward=True, left=True, right=True))\n",
    "State(next_waypoint='forward', actions_legality=ActionsLegality(no_action=True, forward=True, left=False, right=True))\n",
    "State(next_waypoint='forward', actions_legality=ActionsLegality(no_action=True, forward=False, left=False, right=True))\n",
    "State(next_waypoint='forward', actions_legality=ActionsLegality(no_action=True, forward=False, left=False, right=False))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***QUESTION:*** \n",
    "\n",
    "What states have you identified that are appropriate for modeling the smartcab and environment? Why do you believe each of these states to be appropriate for this problem?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***ANSWER:***\n",
    "\n",
    "The determining factors for our state description are what aspects of a state are tied to the rewards for an action from that state. \n",
    "\n",
    "Since rewards rely on whether the learning agent has followed (or not followed) traffic and safety laws and whether it has moved toward the destination as per the planner's `next_waypoint` returned value, we can combine inputs into the following description:\n",
    "```python\n",
    "State = namedtuple('State', ['next_waypoint', 'light',\n",
    "                             'oncoming', 'left', 'right'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPTIONAL:\n",
    "\n",
    "How many states in total exist for the smartcab in this environment? Does this number seem reasonable given that the goal of Q-Learning is to learn and make informed decisions about each state? Why or why not?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER: \n",
    "\n",
    "Every possible combination of values for each state variable (listed below) is:\n",
    "\n",
    "$2 * 3^4 = 162$\n",
    "\n",
    "```python\n",
    "light: 'green', 'red'\n",
    "oncoming: 'forward', 'left', 'right'\n",
    "left: 'forward', 'left', 'right'\n",
    "right: 'forward', 'left', 'right'\n",
    "next_waypoint: 'forward', 'left', 'right'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/smartcab/')\n",
    "import agent as ag\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload(ag)\n",
    "result = ag.grid_search(trials=100,\n",
    "                        epsilons=(.5,),\n",
    "                        decay_divisors=(3000,),\n",
    "                        learning_rates=(.2, .5, .8),\n",
    "                        discounts=(.2, .5, .8),\n",
    "                        only_Q4=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epsilon</th>\n",
       "      <th>decay_divisor</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>discount</th>\n",
       "      <th>avg_missed__Q1</th>\n",
       "      <th>avg_missed_Q2</th>\n",
       "      <th>avg_missed_Q3</th>\n",
       "      <th>avg_missed_Q4</th>\n",
       "      <th>avg_violations_Q1</th>\n",
       "      <th>avg_violations_Q2</th>\n",
       "      <th>avg_violations_Q3</th>\n",
       "      <th>avg_violations_Q4</th>\n",
       "      <th>avg_moves_Q1</th>\n",
       "      <th>avg_moves_Q2</th>\n",
       "      <th>avg_moves_Q3</th>\n",
       "      <th>avg_moves_Q4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.5</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.72</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>21.12</td>\n",
       "      <td>14.60</td>\n",
       "      <td>15.16</td>\n",
       "      <td>13.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.08</td>\n",
       "      <td>18.08</td>\n",
       "      <td>13.24</td>\n",
       "      <td>12.36</td>\n",
       "      <td>14.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.5</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.76</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.12</td>\n",
       "      <td>17.76</td>\n",
       "      <td>13.68</td>\n",
       "      <td>15.52</td>\n",
       "      <td>12.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.08</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.12</td>\n",
       "      <td>15.92</td>\n",
       "      <td>15.56</td>\n",
       "      <td>15.52</td>\n",
       "      <td>15.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.56</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.20</td>\n",
       "      <td>18.80</td>\n",
       "      <td>15.84</td>\n",
       "      <td>16.12</td>\n",
       "      <td>19.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epsilon  decay_divisor  learning_rate  discount  avg_missed__Q1  \\\n",
       "3      0.5           3000            0.5       0.2            0.16   \n",
       "0      0.5           3000            0.2       0.2            0.28   \n",
       "7      0.5           3000            0.8       0.5            0.24   \n",
       "4      0.5           3000            0.5       0.5            0.12   \n",
       "2      0.5           3000            0.2       0.8            0.32   \n",
       "\n",
       "   avg_missed_Q2  avg_missed_Q3  avg_missed_Q4  avg_violations_Q1  \\\n",
       "3           0.00           0.00            0.0               1.72   \n",
       "0           0.00           0.00            0.0               2.20   \n",
       "7           0.00           0.00            0.0               1.76   \n",
       "4           0.08           0.08            0.0               2.08   \n",
       "2           0.00           0.08            0.0               2.56   \n",
       "\n",
       "   avg_violations_Q2  avg_violations_Q3  avg_violations_Q4  avg_moves_Q1  \\\n",
       "3               0.28               0.04               0.04         21.12   \n",
       "0               0.28               0.04               0.08         18.08   \n",
       "7               0.52               0.00               0.12         17.76   \n",
       "4               0.52               0.12               0.12         15.92   \n",
       "2               0.40               0.08               0.20         18.80   \n",
       "\n",
       "   avg_moves_Q2  avg_moves_Q3  avg_moves_Q4  \n",
       "3         14.60         15.16         13.60  \n",
       "0         13.24         12.36         14.56  \n",
       "7         13.68         15.52         12.96  \n",
       "4         15.56         15.52         15.08  \n",
       "2         15.84         16.12         19.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display(result.groupby(list(result.columns[:4])).sum())\n",
    "# result.describe()\n",
    "display(result.sort_values(['avg_missed_Q4', 'avg_violations_Q4', 'avg_moves_Q4']).head())\n",
    "# display(result.sort_values(['avg_violations_Q4', 'avg_missed_Q4', 'avg_moves_Q4']).head())\n",
    "# display(result.sort_values(['avg_moves_Q4', 'avg_missed_Q4', 'avg_violations_Q4']).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement a Q-Learning Driving Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Instructions:***\n",
    "With your driving agent being capable of interpreting the input information and having a mapping of environmental states, your next task is to implement the Q-Learning algorithm for your driving agent to choose the best action at each time step, based on the Q-values for the current state and action. Each action taken by the **smartcab** will produce a reward which depends on the state of the environment. The Q-Learning driving agent will need to consider these rewards when updating the Q-values. Once implemented, set the simulation deadline enforcement `enforce_deadline` to `True`. Run the simulation and observe how the **smartcab** moves about the environment in each trial.\n",
    "\n",
    "The formulas for updating Q-values can be found in [this] video.\n",
    "\n",
    "[this]:https://classroom.udacity.com/nanodegrees/nd009/parts/0091345409/modules/e64f9a65-fdb5-4e60-81a9-72813beebb7e/lessons/5446820041/concepts/6348990570923"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Notes:***\n",
    "\n",
    "Pseudocode of algorithm:\n",
    "\n",
    "1. determine state value\n",
    "- use probaility $\\epsilon$ to decide whether to take random action or use best according to q-value\n",
    "- decay $\\epsilon$\n",
    "- take action, get reward\n",
    "- determine new state value\n",
    "- use $<s, a, r, s'>$ to update Q value (of $s$ with $a$)\n",
    "\n",
    "NOTE: Modified version of above to so an accurate determination of the next state from the next step. Essentially it looks back to previous state after determine $s'$\n",
    "\n",
    "Good discussion:\n",
    "https://discussions.udacity.com/t/qtable-content-example/178397/20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***QUESTION:***\n",
    "\n",
    "What changes do you notice in the agent's behavior when compared to the basic driving agent when random actions were always taken? Why is this behavior occurring?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***ANSWER:***\n",
    "\n",
    "As the learning agent progresses through the trials, it begins to take actions toward the destination more frequently. These actions also begin to follow traffic rules more often.\n",
    "\n",
    "This behavior changes are occur because:\n",
    "1. The agent is making more moves toward the destination (positive rewards).\n",
    "- The agent is driving more safely (negative rewards)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve the Q-Learning Driving Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Instructions:***\n",
    "\n",
    "Your final task for this project is to enhance your driving agent so that, after sufficient training, the **smartcab** is able to reach the destination within the allotted time safely and efficiently. Parameters in the Q-Learning algorithm, such as the learning rate (`alpha`), the discount factor (`gamma`) and the exploration rate (`epsilon`) all contribute to the driving agent’s ability to learn the best action for each state. To improve on the success of your **smartcab**:\n",
    "\n",
    "- Set the number of trials, `n_trials`, in the simulation to 100.\n",
    "- Run the simulation with the deadline enforcement `enforce_deadline` set to `True` (you will need to reduce the update delay `update_delay` and set the `display` to `False`).\n",
    "- Observe the driving agent’s learning and **smartcab’s** success rate, particularly during the later trials.\n",
    "- Adjust one or several of the above parameters and iterate this process.\n",
    "\n",
    "This task is complete once you have arrived at what you determine is the best combination of parameters required for your driving agent to learn successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***QUESTION:***\n",
    "\n",
    "Report the different values for the parameters tuned in your basic implementation of Q-Learning. For which set of parameters does the agent perform best? How well does the final driving agent perform?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***ANSWER:***\n",
    "\n",
    "_Tuned parameters:_\n",
    "```python\n",
    "self.epsilon = 0.5  # tested .1, .3, .5 and .8\n",
    "self.decay_rate = self.epsilon / 400  # adjusted with eps\n",
    "\n",
    "self.learning_rate = 0.2  # alpha, tested .1 and .3\n",
    "self.discount = 0.2  # gamma\n",
    "```\n",
    "\n",
    "_Details of tuning for `self.epsilon`_:\n",
    "\n",
    "- `self.epsilon = 0.1`\n",
    "    - 64.7% of trials where destination is reached\n",
    "- `self.epsilon = 0.3`\n",
    "    - 92.8% of trials where destination is reached\n",
    "- `self.epsilon = 0.5`\n",
    "    - 96.5% of trials where destination is reached\n",
    "- `self.epsilon = 0.8`\n",
    "    - 95% of trials where destination is reached\n",
    "    \n",
    "\n",
    "With these parameters assigned above, our **smartcab** will reach its destination about 95 times out of our 100 trials with the deadline enforced. (Average determined from sample of 10 batches of each set of 100 trials.)\n",
    "\n",
    "Negative rewards in the latter 3/4 of the 100 trials were rare, in the range of 0 to 3 instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***QUESTION:***\n",
    "\n",
    "Does your agent get close to finding an optimal policy, i.e. reach the destination in the minimum possible time, and not incur any penalties? How would you describe an optimal policy for this problem?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***ANSWER:***\n",
    "\n",
    "An optimal policy would always take actions to move toward the destination unless this action would violate traffic laws. In this case, the optimal policy would then determine the next best move (with regard to the direction of the destination) that does _not_ violate traffic laws. I.e. first priority is safety, second is reaching the destination.\n",
    "\n",
    "I would say my agent is getting close to the optimal policy. As mentioned in the previous question, with appropriate parameters, the learning agent almost always reaches the destination by the deadline after the first few trials. In addition, when measuring the frequency of negative rewards (i.e. traffic violations), I found very few occured after the agent learned over 25 trials. However, this 0 to 3 range for violations for the latter 75 trials is much too high according to our definition of an optimal policy which prioritizes safety over reaching the destination. \n",
    "\n",
    "Also, while the learning agent consistently reached the destination by the deadline, often it made non-optimal moves with respect to the destination. This caused it to travel longer than necessary, even while driving safely. Since there is no default penalty for every move (or non-move), the agent is not incentivized to minimize travel time. Perhaps, as well, the penalty for traffic violations should lower than a separate reward for one that causes a collision. These penalties could also be increased to ensure that no violation occur in later trials.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python2]",
   "language": "python",
   "name": "conda-env-python2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
